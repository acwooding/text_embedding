{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "\n",
    "from src.data import datasets, utils, Dataset\n",
    "from src.data.datasets import (build_dataset_dict, fetch_and_unpack, fetch_text_file, read_space_delimited,\n",
    "                                   load_dataset)\n",
    "from src.data.utils import hash_file, list_dir, head_file, normalize_labels\n",
    "from src.paths import interim_data_path, raw_data_path, processed_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding and Processing Natural Datasets\n",
    "## The LVQ-PAK Finnish Phonetic dataset\n",
    "\n",
    "The Learning Vector Quantization project includes a simple Finnish phonetic dataset\n",
    "consisting 20-dimensional data and their associated targets. Let's explore this dataset and\n",
    "add it to our global `datasets.json` so it can be unpacked and processed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='lvq-pak'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the tarfile and build the dataset dictionary for it. If we know the hash of this file, we should include it here. If not, one will be computed from this download and used for comparison on subsequent downloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the source code package\n",
    "lvq_pak = build_dataset_dict(url=\"http://www.cis.hut.fi/research/lvq_pak/lvq_pak-3.1.tar\")\n",
    "lvq_pak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **name** field can be used to indicate the type of datafile being downloaded. Usually, this is just informational. However, if you specify names `DESCR` or `LICENSE`, the downloaded (text) file will be used as the dataset description and license text, respectively.\n",
    "\n",
    "Usually you will want to give these unique names, so they don't clash with other downloaded files. (e.g. \"LICENSE.txt\" is a terrible name to use). We use the **file_name** option for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr = build_dataset_dict(url='http://www.cis.hut.fi/research/lvq_pak/README', file_name=f'{dataset_name}.readme',\n",
    "                       name='DESCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the files have been downloaded to the RAW directory\n",
    "list_dir(raw_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the complete set of files into a URL list and use this to build our json file entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = [lvq_pak, descr]\n",
    "url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newds_dict = datasets.add_dataset_by_urllist(dataset_name, url_list)\n",
    "pprint(newds_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that a generic `load_function` (`new_dataset`) has been used to process the data. This does nothing more than populates the DESCR and LICENSE fields (if possible), creating an otherwise empty `Dataset` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, call the (generic) load function and notice that the LICENSE and DESCR have been set\n",
    "dset = newds_dict['load_function']()\n",
    "type(dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "license = getattr(dset, 'LICENSE', None)\n",
    "print(license)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets should *always* have an explicit license. Reading the project documentation, we see a license in one of the textfiles. We can extract and use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "license_txt = '''\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*                              LVQ_PAK                                 *\n",
    "*                                                                      *\n",
    "*                                The                                   *\n",
    "*                                                                      *\n",
    "*                   Learning  Vector  Quantization                     *\n",
    "*                                                                      *\n",
    "*                          Program  Package                            *\n",
    "*                                                                      *\n",
    "*                   Version 3.1 (April 7, 1995)                        *\n",
    "*                                                                      *\n",
    "*                          Prepared by the                             *\n",
    "*                    LVQ Programming Team of the                       *\n",
    "*                 Helsinki University of Technology                    *\n",
    "*           Laboratory of Computer and Information Science             *\n",
    "*                Rakentajanaukio 2 C, SF-02150 Espoo                   *\n",
    "*                              FINLAND                                 *\n",
    "*                                                                      *\n",
    "*                      Copyright (c) 1991-1995                         *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "*                                                                      *\n",
    "*  NOTE: This program package is copyrighted in the sense that it      *\n",
    "*  may be used for scientific purposes. The package as a whole, or     *\n",
    "*  parts thereof, cannot be included or used in any commercial         *\n",
    "*  application without written permission granted by its producents.   *\n",
    "*  No programs contained in this package may be copied for commercial  *\n",
    "*  distribution.                                                       *\n",
    "*                                                                      *\n",
    "*  All comments concerning this program package may be sent to the     *\n",
    "*  e-mail address 'lvq@nucleus.hut.fi'.                                *\n",
    "*                                                                      *\n",
    "************************************************************************\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list += [datasets.build_dataset_dict(from_txt=license_txt, file_name=f'{dataset_name}.license', name='LICENSE')]\n",
    "url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, reload the dataset from scratch and check that the license is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newds_dict = datasets.add_dataset_by_urllist(dataset_name, url_list)\n",
    "dset = datasets.load_dataset(dataset_name)\n",
    "print(dset.LICENSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data\n",
    "The next step is to write the importer that actually processes the data we will be using for this dataset.\n",
    "\n",
    "The important things to generate are `data` and `target` entries. A `metadata` is optional, but recommended if you want to save additional information about the dataset.\n",
    "\n",
    "Usually, this functionality gets bundled up into a function and added to `datasets.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack the file\n",
    "untar_dir = fetch_and_unpack(dataset_name)\n",
    "unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "list_dir(unpack_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, the training and test datsets are stored in files named `ex1.dat` and `ex2.dat` respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile_train = unpack_dir / 'ex1.dat'\n",
    "datafile_test = unpack_dir / 'ex2.dat'\n",
    "\n",
    "datafile_train.exists() and datafile_test.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, the data format is space-delimited, with the class label included as the last column. Let's have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(head_file(datafile_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the datafile consists of 1 line containing the dimension of the data, a comment, and then 21 space-delimited columns, the final column being the target class label. \n",
    "\n",
    "**Note:** We have to be a little careful importing the data, because '#' is used both as the comment delimiter, and as a class label.\n",
    "\n",
    "Fortunately, we have a helper function for this. We will get a little cheeky and skip the first 2 lines (hoping there are no other comments). The documentation also says ther are 1962 entries in each of the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = read_space_delimited(datafile_train, skiprows=[0,1])\n",
    "data2, target2 = read_space_delimited(datafile_test, skiprows=[0])\n",
    "\n",
    "data.shape, target.shape, data2.shape, target2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work, so let's wrap this functionality up into a processing function.\n",
    "By convention, the function takes a `dataset_name`, and any other options that may be useful for reading the data, and returns a dictionary that matches the `Dataset` constructor signature.\n",
    "\n",
    "We will place this function in `localdata.py`, (and add it to `__all__`) to make it visible to our dataset code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file ../src/data/localdata.py\n",
    "\"\"\"\n",
    "Custom dataset processing/generation functions should be added to this file\n",
    "\"\"\"\n",
    "\n",
    "from src.data.utils import read_space_delimited, normalize_labels\n",
    "from src.paths import interim_data_path\n",
    "import numpy as np\n",
    "\n",
    "__all__ = ['process_lvq_pak']\n",
    "\n",
    "def process_lvq_pak(dataset_name='lvq-pak', kind='all', numeric_labels=True, metadata=None):\n",
    "    \"\"\"\n",
    "    kind: {'test', 'train', 'all'}, default 'all'\n",
    "    numeric_labels: boolean (default: True)\n",
    "        if set, target is a vector of integers, and label_map is created in the metadata\n",
    "        to reflect the mapping to the string targets\n",
    "    \"\"\"\n",
    "    \n",
    "    untar_dir = interim_data_path / dataset_name\n",
    "    unpack_dir = untar_dir / 'lvq_pak-3.1'\n",
    "\n",
    "    if kind == 'train':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "    elif kind == 'test':\n",
    "        data, target = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "    elif kind == 'all':\n",
    "        data1, target1 = read_space_delimited(unpack_dir / 'ex1.dat', skiprows=[0,1])\n",
    "        data2, target2 = read_space_delimited(unpack_dir / 'ex2.dat', skiprows=[0])\n",
    "        data = np.vstack((data1, data2))\n",
    "        target = np.append(target1, target2)\n",
    "    else:\n",
    "        raise Exception(f'Unknown kind: {kind}')\n",
    "\n",
    "    if numeric_labels:\n",
    "        if metadata is None:\n",
    "            metadata = {}\n",
    "        mapped_target, label_map = normalize_labels(target)\n",
    "        metadata['label_map'] = label_map\n",
    "        target = mapped_target\n",
    "\n",
    "    dset_opts = {\n",
    "        'dataset_name': dataset_name,\n",
    "        'data': data,\n",
    "        'target': target,\n",
    "        'metadata': metadata\n",
    "    }\n",
    "    return dset_opts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure this works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.localdata import process_lvq_pak\n",
    "\n",
    "for kind in ['train', 'test', 'all']:\n",
    "    dset_opts = process_lvq_pak(kind=kind)\n",
    "    dset = Dataset(**dset_opts)\n",
    "    print(f'{kind}: data={dset.data.shape} target={dset.target.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This all looks good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.add_dataset_from_function(dataset_name, process_lvq_pak);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, re-load the dataset and save a copy of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvq = load_dataset(dataset_name)\n",
    "print(str(lvq))\n",
    "lvq.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved data is stored in the `processed_data_path`. An copy of just the metadata is also stored, so that it may be quickly checked without loading the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dir(processed_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
